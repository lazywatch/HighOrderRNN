import itertools
import random
import argparse
import numpy as np
from keras.preprocessing.sequence import pad_sequences
# python data_lstar.py --gram 7
parser = argparse.ArgumentParser(description='Training Sets Generated by Lstar Paper for Tomita grammars')
parser.add_argument('--gram', type=int, default=5, help='index of Tomita grammars')
parser.add_argument('--dseed', type=int, default=123, help='random seed for generating data')

args = parser.parse_args()

if args.dseed < 0:
    args.dseed = 123

random.seed(args.dseed)

def mean(num_list):
    return sum(num_list)*1.0/len(num_list)

def n_words_of_length(n,length,alphabet):
    if 50*n >= pow(len(alphabet),length):
        res = all_words_of_length(length, alphabet)
        random.shuffle(res)
        return res[:n]
    #else if 50*n < total words to be found, i.e. looking for 1/50th of the words or less
    res = set()
    while len(res)<n:
        word = ""
        for _ in range(length):
            word += random.choice(alphabet)
        res.add(word)
    return list(res)

def all_words_of_length(length,alphabet):
    return [''.join(list(b)) for b in itertools.product(alphabet, repeat=length)]


def compare(network,classifier,length,num_examples=1000,provided_samples=None):
    if not None == provided_samples:
        words = provided_samples
    else:
        words = n_words_of_length(num_examples,length,network.alphabet)
    disagreeing_words = [w for w in words if not (network.classify_word(w) == classifier.classify_word(w))]
    return 1-(len(disagreeing_words)/len(words)), disagreeing_words

def map_nested_dict(d,mapper):
    if not isinstance(d,dict):
        return mapper(d)
    return {k:map_nested_dict(d[k],mapper) for k in d}

class MissingInput(Exception):
    pass

def tomita_1(word):
    return not "0" in word

def tomita_2(word):
    return word=="10"*(int(len(word)/2))

import re
_not_tomita_3 = re.compile("((0|1)*0)*1(11)*(0(0|1)*1)*0(00)*(1(0|1)*)*$")
# *not* tomita 3: words containing an odd series of consecutive ones and then later an odd series of consecutive zeros
# tomita 3: opposite of that
def tomita_3(w):
    return None == _not_tomita_3.match(w) #complement of _not_tomita_3

def tomita_4(word):
    return not "000" in word

def tomita_5(word):
    return (word.count("0")%2 == 0) and (word.count("1")%2 == 0)

def tomita_6(word):
    return ((word.count("0")-word.count("1"))%3) == 0

def tomita_7(word):
    return word.count("10") <= 1

def make_train_set_for_target(target,alphabet,lengths=None,max_train_samples_per_length=300,search_size_per_length=1000,provided_examples=None):
    train_set = {}
    if None == provided_examples:
        provided_examples = []
    if None == lengths:
        lengths = list(range(15)) + [15,20,25,30]
    for l in lengths:
        samples = [w for w in provided_examples if len(w)==l]
        samples += n_words_of_length(search_size_per_length,l,alphabet)
        pos = [w for w in samples if target(w)]
        neg = [w for w in samples if not target(w)]
        pos = pos[:int(max_train_samples_per_length/2)]
        neg = neg[:int(max_train_samples_per_length/2)]
        minority = min(len(pos),len(neg))
        pos = pos[:minority+20]
        neg = neg[:minority+20]
        train_set.update({w:True for w in pos})
        train_set.update({w:False for w in neg})

    print("made train set of size:",len(train_set),", of which positive examples:",
        len([w for w in train_set if train_set[w]==True]))

    return train_set

#curriculum
def mixed_curriculum_train(rnn,train_set,outer_loops=3,stop_threshold=0.001,learning_rate=0.001,
    length_epochs=5,random_batch_epochs=100,single_batch_epochs=100,random_batch_size=20):
    lengths = sorted(list(set([len(w) for w in train_set])))
    for _ in range(outer_loops):
        for l in lengths:
            training = {w:train_set[w] for w in train_set if len(w)==l}
            if len(set([training[w] for w in training])) <= 1: #empty, or length with only one classification
                continue
            rnn.train_group(training,length_epochs,show=False,loss_every=20,stop_threshold=stop_threshold,
                            learning_rate=learning_rate,batch_size=None,print_time=False)
        # all together but in batches
        if rnn.finish_signal == rnn.train_group(train_set,random_batch_epochs,show=True,loss_every=20,
                                                stop_threshold = stop_threshold,
                                                learning_rate=learning_rate,
                                                batch_size=random_batch_size,print_time=False):
            break
        # all together in one batch
        if rnn.finish_signal == rnn.train_group(train_set,single_batch_epochs,show=True,loss_every=20,
                                                stop_threshold = stop_threshold,
                                                learning_rate=learning_rate,batch_size=None,print_time=False): 
            break
    print("classification loss on last batch was:",rnn.all_losses[-1])


def prepare_x_m_y(data_set, curriculum=False):
    x = []
    m = []
    y = []
    if curriculum:
        lengths = sorted(list(set([len(w) for w in data_set])))
        max_len = lengths[-1]
        for l in lengths:
            for w in data_set:
                if len(w) == l:
                    x.append([int(bit) for bit in w])
                    m.append(np.ones_like(x[-1]))
                    y.append([1 if data_set[w] else 0])

            #x_tmp = [[int(bit) for bit in w] for w in data_set if len(w)==l]
            #m_tmp = [np.ones_like(w) for w in x_tmp]
            #y_tmp = [1 if data_set[w] else 0 for w in data_set if len(w) == l]
    else:
        max_len = 0
        for key, value in data_set.iteritems():
            seq_x = [int(bit) for bit in key]
            seq_y = [1] if value else [0]
            x.append(seq_x)
            m.append(np.ones_like(seq_x))
            y.append(seq_y)
            max_len = len(seq_x) if len(seq_x) > max_len else max_len



    x = pad_sequences(x, maxlen=max_len, dtype='int32', padding='post', value=0)
    m = pad_sequences(m, maxlen=max_len, dtype='int32', padding='post', value=0)
    y = np.array(y, dtype='int32').squeeze(axis=1)

    return x, y, m


if args.gram == 1:
    target = tomita_1
    train_lengths = list(range(14)) + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(4,30,3))
    test_lengths = list(range(30))
elif args.gram == 2:
    target = tomita_2
    train_lengths = list(range(14)) + [16, 19, 22]
    max_train_samples_per_length = 1000
    val_lengths = list(range(4,30,3))
    test_lengths = list(range(30))
elif args.gram == 3:
    target = tomita_3
    train_lengths = list(range(14)) + [16, 19, 22]
    max_train_samples_per_length = 500
    val_lengths = list(range(4,30,3))
    test_lengths = list(range(30))
elif args.gram == 4:
    target = tomita_4
    train_lengths = list(range(14)) + [16, 19, 22]
    max_train_samples_per_length = 320
    val_lengths = list(range(4,30,3))
    test_lengths = list(range(30))
elif args.gram == 5:
    target = tomita_5
    train_lengths = list(range(14)) + [16, 19, 22]
    max_train_samples_per_length = 800#340
    val_lengths = list(range(4,23,3))
    test_lengths = list(range(30))
elif args.gram == 6:
    target = tomita_6
    train_lengths = list(range(14)) + [15, 16, 17, 18, 19, 20]#[16, 19, 22, 25, 28]#[15, 16, 17, 18, 19, 20]
    max_train_samples_per_length = 800#280
    val_lengths = list(range(4,23,3))
    test_lengths = list(range(30))
elif args.gram == 7:
    target = tomita_7
    train_lengths = list(range(14)) + [16, 19, 22]
    max_train_samples_per_length = 1000


max_val_samples_per_length = 100
max_test_samples_per_length = 4000



alphabet = "01"

train_set = make_train_set_for_target(target=target, alphabet=alphabet, lengths=train_lengths, max_train_samples_per_length=max_train_samples_per_length)
val_set = make_train_set_for_target(target=target, alphabet=alphabet, lengths=val_lengths, max_train_samples_per_length=max_val_samples_per_length)
test_set = make_train_set_for_target(target=target, alphabet=alphabet, lengths=test_lengths, max_train_samples_per_length=max_test_samples_per_length)

if args.gram == 5 or args.gram == 6:
    train_x, train_y, train_m = prepare_x_m_y(train_set, True)
else:
    train_x, train_y, train_m = prepare_x_m_y(train_set, False)
val_x, val_y, val_m = prepare_x_m_y(val_set)
test_x, test_y, test_m = prepare_x_m_y(test_set)

train_val_test_file = ''.join(('./gram', str(args.gram), '/train_val_test_data_lstar.npz'))
np.savez(train_val_test_file, train_x=train_x, train_m=train_m, train_y=train_y,
         val_x=val_x, val_m=val_m, val_y=val_y, test_x=test_x, test_m=test_m, test_y=test_y)
